---
title: "Statistical Tests in R"
author: "yilin"
date: "2021/11/20"
output:
  html_document:
    df_print: paged
  pdf_document:
    latex_engine: xelatex
  word_document: default
---
# Content
# Part1 one sample
```{r}
library(tidyverse)
library(tinytex)
library(knitr)
library(UsingR)
library(BSDA)
#{stats}
#one-sample t test
#Is the mean of a sample significantly different from a hypothesized mean?
#1:the sample observations are normally distributed; 2 the population standard deviation is unknown.
x <- c(50,60,20,75,77,85,20)
t.test(x,mu=60, alternative = "two.sided", conf.level = 0.95)#alternative less or greater
# ONE SAMPLE WILCOXON SIGNED RANK TEST
#Is the median of a sample significantly different from a hypothesized value? When to use the test?
#It is a nonparametric test and therefore requires no assumption for the sample distribution. #example A questionnaire with a seven point scale
x <- c(1.9,59.3,14.2,32.9,69.1,23.1,79.3,51.9,39.2,41.8,102.3) 
wilcox.test (x,mu=60, alternative = "two.sided")
#annual rate of change/nutrient values/time series
x<-c(12,2,17,25,52,8,1,12) 
UsingR::simple.median.test(x, median = 20)
BSDA::SIGN.test(x, md = 20,alternative = "two.sided", conf.level=0.95)
```
# Part2 **two independent sample**#
```{r,include=TRUE}
#Two sample test 
#Is the difference between the mean of two samples significantly different from zero?
#1.observations are normally distributed;2.the sample variances are equal
#independent two groups,such as gender
x<-c(0.795,0.864,0.841,0.683,0.777,0.720)
y<-c(0.765,0.735,1.003,0.778,0.647,0.740,0.126) 
t.test(x,y, alternative = "two.sided", var.equal=TRUE)
#when var.equal=FALSE 检验方法就WELCH T-TEST 
#observations are not normally distributed/to test median /not sensitive to outliers
wilcox.test(x,y, alternative = "two.sided")#Mann–Whitney–Wilcoxon test/Wilcoxon rank sum test
#PAIRWISE T-TEST
#Is the difference between the mean of three or more samples significantly different from zero?
#1 normally distributed;2.equal variances
sample_1 <- c(2.9, 3.5, 2.8, 2.6, 3.7)
sample_2 <- c(3.9, 2.5, 4.3, 2.7) 
sample_3<- c(2.9, 2.4, 3.8, 1.2, 2.0) 
sample <- c(sample_1, sample_2, sample_3) 
g <- factor(rep(1:3, c(5, 4, 5)), labels = c("sample_1", " sample_2", " sample_3"))
pairwise.t.test(sample, g, p.adjust.method ="bonferroni", pool.sd = TRUE,alternative = "two.sided")#3 Pvalue present
```
#Part3 **paired sample**#
```{r,include=T}
#Paired t test
#measured twice, before and after a treatment, matched pairs
##1.paired;2.observation normally distributed
x <- c(16,20,21,22,23,22,27,25,27,28)#case
y <- c(19,22,24,24,25,25,26,26,28,32)#control
t.test(x,y, alternative = "two.sided", paired =TRUE)
#observation not normally distributed
x <- c(1.83, 0.50, 1.62, 2.48, 1.68, 1.88, 1.55, 3.06, 1.30)
y <- c(0.878, 0.647, 0.598, 2.05, 1.06, 1.29, 1.06, 3.14, 1.29)
#MATCHED PAIRS WILCOXON TEST
wilcox.test(x, y, paired = TRUE, alternative = "two.sided")
#To test sample medians are equal type; continuous
BSDA::SIGN.test(x,y, alternative = "two.sided")
#PAIRWISE PAIRED T-TEST FOR DIFFERENCE
#population with a normal distribution;more than two paired group
sample_1 <- c(2.9, 3.5, 2.8, 2.6, 3.7,4.0) 
sample_2 <- c(3.9, 2.5, 4.3, 2.7,2.6,3.0) 
sample_3<- c(2.9, 2.4, 3.8, 1.2, 2.0,1.97) 
sample <- c(sample_1, sample_2, sample_3)
g <- factor(rep(1:3, c(6,6,6)), labels = c("sample_1", " sample_2", " sample_3"))
pairwise.t.test(sample,g, p.adjust.method ="holm", paired=
TRUE,alternative = "two.sided")
#can not assumed to be drawn from a population with a normal distribution;more than two paired group
```
Bonferroni(Control FWER, Single-Step Procedure)对每个P值进行相同的调整
Holm(Control FWER, Sequential Procedure)逐步调整P
Benjamini, Hochberg and Yetkutieli(BHY) (Control FDR, Sequential Procedure*
#Part4 **random test**
```{r include=T}
library(lawstat)
library(tseries)
#WALD-WOLFOWITZ RUNS TEST FOR DICHOTOMOUS DATA
#Is the sequence of binary events in a sample randomly distributed?
x<-factor(c(1,0,0,0,0,0,0,0,1,1,1,1,0,1,1,1,1,1,1,1,1,0,0,0,0,0))
tseries::runs.test(x,alternative ="two.sided")#non-random 

#WALD-WOLFOWITZ RUNS TEST FOR CONTINUOUS DATA
#e.g.whether infections were randomly distributed across time
y<-c(1.8,2.3,3.5,4,5.5,6.3,7.2,8.9,9.1)
lawstat::runs.test(y,alternative="positive.correlated")#negative.correlated
#autocorrelation
y<-c(-82.29,-31.14,136.58,85.42,42.96,-122.72,0.59,55.77,117.62,-10.95,- 211.38,-304.02,30.72,238.19,140.98,18.88,-48.21,-63.7)
Box.test (y, lag = 3,type = "Ljung-Box")#lag:ln(n), where n is the number of observations in the sample
```
#Part5 **variance**
```{r include=T}
library(PairedData)
library(car)
library(outliers)
#F-TEST OF EQUALITY OF VARIANCES
#test two independent samples have the same variance; normality; two samples
x=c(10.8,11.0,10.4,10.3,11.3) 
y=c (10.8,10.6,11,10.9,10.9,10.7,1.8)
var.test(x,y, ratio=1, alternative="two.sided",conf.level = 0.95)#ratio is the hypothesized ratio of variance下面这个也可
PairedData::Var.test(x,y, ratio=1, alternative="two.sided",paired =F,conf.level = 0.95)
#BARTLETT TEST FOR HOMOGENEITY OF VARIANCE
#multiple independent samples have the same variance; Normality ;for k samples
count_data<-c(250,260,230,270,310,330,280,360,250,230,220,260,340,270,300,320,250,240,270,290)
sample<-c("A","A","A","A","B","B","B","B","C","C","C","C","D","D","D","D","E","E","E","E")
data<-data.frame((list(count= count_data, sample=sample)))
bartlett.test(data$count, data$sample)
bartlett.test(count ~ sample, data = data)
#FLIGNER-KILLEEN TEST
##multiple independent samples have the same variance; 不正态分布也可以 ;for k samples
fligner.test (count ~ sample, data = data)
#LEVENE'S TEST
#multiple independent samples have the same variance;不正态分布也可以 ;for k samples
car::leveneTest (count ~ sample, data = data)
#To test the null hypothesis that mulyiple independent samples have the same variance. The test is more robust to departures from normality than Bartlet’s test for homogeneity of variances.BROWN-FORSYTHE LEVENE-TYPE TEST
lawstat::levene.test(data$count,data$sample,location="median",correction.method="zero.correction")
#PITMAN-MORGAN TEST
#test two correlated samples have the same variance/normality,e.g.重复测量资料
machine.am=c(10.8,11.0,10.4,10.3,11.3,10.2,11.1) 
machine.pm=c (10.8,10.6,11,10.9,10.9,10.7,1.8)
PairedData::Var.test(machine.am,machine.pm,alternative="two.sided",conf.level = 0.95,paired=T)
#for paired samples
levene.Var.test(machine.am,machine.pm)
#MAUCHLY'S SPHERICITY TEST
#pairs of groups in a repeated measures
dependent_variable <- c (-5, -10, -5, 0, -3, -3, -5, -7, -2, 4, -1, -5, -4, -8, -4,-5,- 12,-7)
mlm <- matrix (dependent_variable, nrow = 6, byrow = TRUE)
mauchly.test (lm (mlm ~1), X =~1)
```
#**Part 6 correlation**
```{r include=T}
library(psych)
#PEARSON’S PRODUCT MOMENT CORRELATION COEFFICIENT T-TEST
#a paired random sample；Normality;joint distribution is bivariate normal;linear.
x <- c(44.4, 45.9, 41.9, 53.3, 44.7, 44.1, 50.7, 45.2, 60.1)
y <- c( 2.6, 3.1, 2.5, 5.0, 3.6, 4.0, 5.2, 2.8, 3.8)
cor.test(x,y,method="pearson",alternative="two.sided",conf.level = 0.95)#密切程度和相关方向
#SPEARMAN RANK CORRELATION TEST
#不符合正态分布；liner
cor.test(x,y,method="spearman",alternative="two.sided")
#KENDALL’S TAU CORRELATION COEFFICIENT TEST
#通常Spearman用于连续变量，而Kendall用于有序分类变量，都是等级相关
cor.test(x,y,method="kendal",alternative="two.sided")
#Z TEST OF THE DIFFERENCE BETWEEN two INDEPENDENT CORRELATIONS
paired.r(0.54,0.89,NULL,17,26,twotailed=TRUE)#null hypothesis:equality of correlations
#DIFFERENCE BETWEEN TWO OVERLAPPING CORRELATION COEFFICIENTS(Steiger’s t-test)
#e.g. “is the correlation of age stronger on neuroticism than on anxiety?
# normally distributed
paired.r(xy=-0.2,xz=-0.28,yz=0.30, 123,twotailed=TRUE)
#DIFFERENCE BETWEEN TWO NON-OVERLAPPING DEPENDENT CORRELATION COEFFICIENTS(Williams's Test)
r.test(187, r12 = -0.11, r34 = 0.06, r23 = 0.41)
#BARTLETT’S TEST OF SPHERICITY
#H0:变量之间是不相关的；在因子分析降维之前用（P<0.001)
set.seed(1234) 
n=1000 
y1 <- rnorm(n) 
y2<- rnorm(n) 
y3<-y1+y2 
data<-matrix(c(y1,y2,y3) , nrow = n, ncol=3, byrow=TRUE,)
correlation.matrix <- cor(data)
cortest.bartlett(correlation.matrix,n)
```
#**Part 7 other distribution significant test and proportion test**
```{r include=T}
#BINOMINAL TEST
#a dichotomous variable differ from the frequencies that are expected under
binom.test(x = 25, n= 30, p = 0.5, alternative = "two.sided", conf.level = 0.95)
binom.test(x = 25, n= 30, p = 0.5, alternative = "greater", conf.level = 0.95)
#ONE SAMPLE PROPORTIONS TEST
prop.test(52,100,p=0.5,alternative = "two.sided", conf.level = 0.95)
#ONE SAMPLE POISSON TEST
poisson.test(6,6.22,alternative="two.sided",conf.level=0.95)
#TWO SAMPLE POISSON TEST
poisson.test(c(10,2),c(20000,17877),alternative="two.sided",conf.level=0.95)
#PAIRWISE COMPARISON OF PROPORTIONS TEST（Pearson’s Chi-Squared test
sample<-rbind(s1=c(95,106),s2=c(181,137),s3=c(76,85),s4=c(13,29),s5=c(11,26),s6=c(201,179))
colnames(sample) <-c("treat1","trea2")
#The test can then be carried out by typing
pairwise.prop.test(sample, p.adjust.method ="holm" )
```
#**Part 8 Chisq test**
```{r include=T}
#CHI-SQUARED TEST FOR LINEAR TREND
#linear regression of the proportions
infected.swimmers <- c( 5,5,33)
all.swimmers <- c( 13,8,37)
prop.trend.test(infected.swimmers, all.swimmers)
#PEARSON’S PAIRED CHI-SQUARED TEST
Table_data<- as.table(rbind(c(15, 30), c(30,15)))
dimnames(Table_data)<-list(gender=c("Male","Female"), party=c("Labour", "Conservative")) 
chisq.test(Table_data , correct = FALSE )
chisq.test(Table_data , correct = TRUE) #Yates' continuity correction,40<n<100
#FISHERS EXACT TEST
fisher.test(Table_data, alternative = "two.sided", conf.level = 0.95)#T<5 or n<40
#COCHRAN-MANTEL-HAENSZEL TEST
#relationship between two categorical variables a?er adjus?ng for control variables控制混杂
Data <-array(c(12, 16, 7, 19,16, 11, 5, 20), dim = c(2, 2, 2), dimnames = list(Treatment = c("Drug", " Placebo"), Response = c("Improved", "No Change"), Sex = c("Male", "Female"))) 
mantelhaen.test(Data, alternative = "two.sided", correct = FALSE, exact = FALSE, conf.level = 0.95)
#MCNEMAR'S TEST
#a paired version of Chi-square test
data<-matrix(c(59, 4, 128, 20), nrow = 2, dimnames = list("chest radiography" = c("positive", "negative"), "sputum culture" = c("positive", "negative")))
mcnemar.test(data)#b+c<20,用fisher,b+c<40 需要Yate's correlation
```
#**Part 9 ANOVA**
The basic idea is to partition the total variability in the responses into two pieces. 
One piece summarizes the variability explained by the model. The other piece summarizes the variability due to error and captured in the residuals.
```{r include=T}
#EQUAL MEANS IN A ONE-WAY LAYOUT WITH EQUAL VARIANCES(ANOVA)
#three or more;independent;Normality;equal variance
Value <- c(2.9, 3.5, 2.8, 2.6, 3.7, 3.9, 2.5, 4.3, 2.7, 2.9, 2.4, 3.8, 1.2, 2.0) 
Sample_Group <- factor(c(rep(1,5),rep(2,4),rep(3,5))) 
data <- data.frame(Sample_Group, Value) 
oneway.test(Value~ Sample_Group, data = data, var.equal = TRUE)
#WELCH-TEST for var.equal is not equal
oneway.test(Value~ Sample_Group, data = data, var.equal = FALSE)
##KRUSKAL WALLIS RANK SUM TEST
#three or more;方差不齐或者不服从正态分布
kruskal.test(Value ~ Sample_Group, data=data) 
#FRIEDMAN’S TEST
#>=3；方差不齐或者不服从正态分布；区组设计
Diet_data<- matrix(c(8, 8, 7, 7, 6, 6, 6, 8, 6, 8, 9, 7, 5, 8, 5, 9, 7, 7,7, 7, 7, 8, 7, 7, 8, 6, 8, 7, 6, 6, 7, 8, 6, 9, 9, 6),nrow = 12, byrow = TRUE, dimnames = list(1 : 12, c("Healthy Balanced", "Low Fat", "Low Carb")))
friedman.test(Diet_data)
#QUADE TEST
#As a simple rule of thumb the Quade test is generally more powerful for a small number of treatments whilst the Friedman test is generally more powerful when the number of treatments is five or more.
quade.test(Diet_data)
```
#**Part 10 distribution**
```{r include=T}
library(moments)
library(fBasics)
library(nortest)
#SHAPIRO-WILK TEST 
sample <-c(-1.441,-0.642,0.243,0.154,-0.325,-0.316,0.337,-0.028,1.359,- 1.67,-0.42,1.02,-1.15,0.69,-1.18,2.22,1,-1.83,0.01,-0.77,-0.75,-1.55,- 1.44,0.58,0.16)
shapiro.test (sample)
dagoTest(sample)
#D??? AGOSTINO TEST OF SKEWNESS
#detecting non-normality caused by asymmetry
#If a distribution has normal kurtosis but is skewed, the test for skewness may be more powerful than the Shapiro-Wilk test, especially if the skewness is mild.
agostino.test(sample, alternative = "two.sided")
#ANSCOMBE-GLYNN TEST OF KURTOSIS
#detecting nonnormality caused by tail heaviness
#the test for kurtosis may be more powerful than the Shapiro-Wilk test, especially if the heavy-tailedness is not extreme.
anscombe.test (sample, alternative = "two.sided" )
#偏度峰度检验也易受异常值的影响
#KOLMOGOROV-SMIRNOV TEST OF NORMALITY
ks.test(sample,"pnorm")
lillie.test(sample)#用于小样???
#ANDERSON-DARLING TEST OF NORMALITY
#normal distribution with unknown mean and variance
ad.test(sample)
#CRAMER-VON MISES TEST
cvm.test(sample) 
#MARDIA'S TEST OF MULTIVARIATE NORMALITY
diff =diff(EuStockMarkets,1)
mardia(diff,plot=F)
#PEARSON CHI SQUARE TEST
pchiTest (sample)
```
**SPSS规定:当样本含量n[3,5000],结果以Shapiro-Wilk(W检???)为准,
当样本含量n>5000结果以Kolmogorov-Smirnov为准
而SAS规定:当样本含量n<2000,结果以Shapiro-Wilk(W检???)为准,
当样本含量n>2000,结果以Kolmogorov-Smirnov(D检???)为准**
#**Part 11 goodness-of-fit
```{r include=T}
library(ADGofTest)
#KOLOMOGOROV SMIRNOV TEST FOR GOODNESS OF FIT
#most sensitive
ks.test(sample,"pnorm")#normal distribution
ks.test(sample,"pexp")#exponential distribution
#pbeta,plnorm,pchisq,ppois,pt,pf,pwilcox,plogis
#ANDERSON-DARLING GOODNESS OF FIT TEST
#giving more weight to the tails of the distribution
ad.test(sample,plnorm)
#TWO-SAMPLE KOLMOGOROV-SMIRNOV TEST
sample1<- c(-2.12, 0.08, -1.59, -0.15, 0.9, -0.7, -0.22, -0.66, -2.14, 0.65, 1.38, 0.27, 3.33, 0.09, 1.45, 2.43, -0.55, -0.68, -0.62, -1.91, 1.11, 0.43, 0.42, 0.09, 0.76)
sample2<- c(0.91, 0.89, 0.6, -1.31, 1.07, -0.11, -1.1, -0.83, 0.8, -0.53, 0.3, 1.05, 0.35, 1.73, 0.09, -0.51, -0.95, -0.29, 1.35, 0.51, 0.66, -0.56, -0.04, 1.03, 1.47)
ks.test(sample1,sample2,alternative="two.sided")
#ANDERSON-DARLING MULTIPLE SAMPLE GOODNESS OF FIT TEST in k distinct samples
```
#**Part 12 outlier**
```{r include=T}
library(outliers)
library(car)
sample<-c(0.189,0.167,0.187,0.183,0.186,0.182,0.181,0.184,0.177)
#DIXON’S Q TEST, n<30小样本，正???
dixon.test(x)
dixon.test(x,opposite=TRUE)
#CHI-SQUARED TEST FOR OUTLIERS
dependent.variable=c(3083,3140,3218,3239,3295,3374,3475,3569,3597,3725,3794,39)               
independent.variable=c(75,78,80,82,84,88,93,97,65,104,109,115)
regression.model<- lm(dependent.variable ~ independent.variable)
residual<-rstudent(regression.model)
chisq.out.test(residual, variance=1)
#BONFERRONI OUTLIER TEST
#based on the largest absolute studentized residual
outlierTest(lm(dependent.variable~independent.variable))
outliers::outlier(residual)
```
#**Part 13 HETEROSCEDASTICITY**
```{r include=T}
#GOLDFELD-QUANDT TEST FOR HETEROSCEDASTICITY
#BREUSCH-PAGAN TEST FOR HETEROSCEDASTICITY
#HARRISON-MCCABE TEST FOR HETEROSKEDASTICITY
```
#**Part 14 regression test**
```{r include=T}
#GRAMBSCH-THERNEAU TEST OF PROPORTIONALITY
#MANTEL-HAENSZEL LOG-RANK TEST
#Are there statistically significant differences between two or more survival curves?
time <- c(13, 18, 28, 26, 21, 22, 24, 25, 10, 13, 15, 16, 17, 19, 25, 32)#months 
status <- c(1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1)
treatment.group <- c(1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2) 
sex <- c(1, 1, 2, 2, 2, 2, 2, 2, 1, 1, 2, 1, 1, 1, 2, 2)
survdiff(Surv(time, status) ~ treatment.group, rho=0)
#PETO AND PETO TEST
#statistically significant differences between two or more survival curves
survdiff(Surv(time, status) ~ treatment.group, rho=1)
```
##**Part 14.1  liner regression test**
```{r include=T}
#simple linear model
#condition:Linearity;Zero Mean;Uniform Spread;Independence;Normality;Randomness
```
**Main reference book:100 Statistical Tests.Feinstein, Alvan R.
Kanji, G. K.**



